{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling TextDataset with TensorFlow APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  1.3.0\n",
      "14 words into vocab.tsv\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lookup\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "print ('TensorFlow Version: ', tf.__version__)\n",
    "\n",
    "# Normally this takes the mean length of the words in the dataset documents\n",
    "MAX_DOCUMENT_LENGTH = 5  \n",
    "# Padding word that is used when a document has less words than the calculated mean length of the words\n",
    "PADWORD = 'ZYXW'\n",
    "\n",
    "# Assume each line to be an document\n",
    "lines = ['Simple',\n",
    "         'Some title', \n",
    "         'A longer title', \n",
    "         'An even longer title', \n",
    "         'This is longer than doc length']\n",
    "\n",
    "# Create vocabulary\n",
    "# min_frequency -> consider a word if and only it repeats for fiven count\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, \n",
    "                                                                     min_frequency=0)\n",
    "vocab_processor.fit(lines)\n",
    "\n",
    "#Create a file and store the words\n",
    "with gfile.Open('vocab_test.tsv', 'wb') as f:\n",
    "    f.write(\"{}\\n\".format(PADWORD))\n",
    "    for word, index in vocab_processor.vocabulary_._mapping.items():\n",
    "      f.write(\"{}\\n\".format(word))\n",
    "    \n",
    "VOCAB_SIZE = len(vocab_processor.vocabulary_)\n",
    "print ('{} words into vocab.tsv'.format(VOCAB_SIZE+1))\n",
    "\n",
    "EMBEDDING_SIZE = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZYXW\r\n",
      "<UNK>\r\n",
      "Simple\r\n",
      "Some\r\n",
      "title\r\n",
      "A\r\n",
      "longer\r\n",
      "An\r\n",
      "even\r\n",
      "This\r\n",
      "is\r\n",
      "than\r\n",
      "doc\r\n",
      "length\r\n"
     ]
    }
   ],
   "source": [
    "! cat vocab_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some title --> [3 4]\n"
     ]
    }
   ],
   "source": [
    "# can use the vocabulary to convert words to numbers\n",
    "table = lookup.index_table_from_file(\n",
    "  vocabulary_file='vocab_test.tsv', num_oov_buckets=1, vocab_size=None, default_value=-1)\n",
    "\n",
    "numbers = table.lookup(tf.constant(lines[1].split()))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Tables needs to be initialized before useing it\n",
    "    tf.tables_initializer().run()\n",
    "    print (\"{} --> {}\".format(lines[1], numbers.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles= [b'Simple' b'Some title' b'A longer title' b'An even longer title'\n",
      " b'This is longer than doc length'] (5,)\n",
      "--------------------------------------------------------\n",
      "words= SparseTensorValue(indices=array([[0, 0],\n",
      "       [1, 0],\n",
      "       [1, 1],\n",
      "       [2, 0],\n",
      "       [2, 1],\n",
      "       [2, 2],\n",
      "       [3, 0],\n",
      "       [3, 1],\n",
      "       [3, 2],\n",
      "       [3, 3],\n",
      "       [4, 0],\n",
      "       [4, 1],\n",
      "       [4, 2],\n",
      "       [4, 3],\n",
      "       [4, 4],\n",
      "       [4, 5]]), values=array([b'Simple', b'Some', b'title', b'A', b'longer', b'title', b'An',\n",
      "       b'even', b'longer', b'title', b'This', b'is', b'longer', b'than',\n",
      "       b'doc', b'length'], dtype=object), dense_shape=array([5, 6]))\n",
      "--------------------------------------------------------\n",
      "dense= [[b'Simple' b'ZYXW' b'ZYXW' b'ZYXW' b'ZYXW' b'ZYXW']\n",
      " [b'Some' b'title' b'ZYXW' b'ZYXW' b'ZYXW' b'ZYXW']\n",
      " [b'A' b'longer' b'title' b'ZYXW' b'ZYXW' b'ZYXW']\n",
      " [b'An' b'even' b'longer' b'title' b'ZYXW' b'ZYXW']\n",
      " [b'This' b'is' b'longer' b'than' b'doc' b'length']] (?, ?)\n",
      "--------------------------------------------------------\n",
      "numbers= [[ 2  0  0  0  0  0]\n",
      " [ 3  4  0  0  0  0]\n",
      " [ 5  6  4  0  0  0]\n",
      " [ 7  8  6  4  0  0]\n",
      " [ 9 10  6 11 12 13]] (?, ?)\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "sliced= [[ 2  0  0  0  0]\n",
      " [ 3  4  0  0  0]\n",
      " [ 5  6  4  0  0]\n",
      " [ 7  8  6  4  0]\n",
      " [ 9 10  6 11 12]] (?, 5)\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# string operations\n",
    "# Array of Docs -> Split it into Tokens/words \n",
    "#               -> Convert it into Dense Tensor apending PADWORD\n",
    "#               -> Table lookup \n",
    "#               -> Slice it to MAX_DOCUMENT_LENGTH\n",
    "titles = tf.constant(lines)\n",
    "words = tf.string_split(titles)\n",
    "densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "numbers = table.lookup(densewords)\n",
    "\n",
    "##Following extrasteps are taken care by above 'table.lookup'\n",
    "\n",
    "# now pad out with zeros and then slice to constant length\n",
    "# padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
    "# this takes care of documents with zero length also\n",
    "# padded = tf.pad(numbers, padding)\n",
    "\n",
    "sliced = tf.slice(numbers, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.tables_initializer().run()\n",
    "    print (\"titles=\", titles.eval(), titles.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"words=\", words.eval())\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"dense=\", densewords.eval(), densewords.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"numbers=\", numbers.eval(), numbers.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "#     print (\"padding=\", padding.eval(), padding.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "#     print (\"padded=\", padded.eval(), padded.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"sliced=\", sliced.eval(), sliced.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    \n",
    "    with tf.device('/cpu:0'), tf.name_scope(\"embed-layer\"):\n",
    "        # layer to take the words and convert them into vectors (embeddings)\n",
    "        # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "        # maps word indexes of the sequence into\n",
    "        # [batch_size, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE].\n",
    "        word_vectors = tf.contrib.layers.embed_sequence(sliced,\n",
    "                                                  vocab_size=VOCAB_SIZE,\n",
    "                                                  embed_dim=EMBEDDING_SIZE)\n",
    "\n",
    "        # [?, self.MAX_DOCUMENT_LENGTH, self.EMBEDDING_SIZE]\n",
    "        tf.logging.debug('words_embed={}'.format(word_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References: \n",
    "- https://medium.com/towards-data-science/how-to-do-text-classification-using-tensorflow-word-embeddings-and-cnn-edae13b3e575\n",
    "- https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/textclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Estimators Inputs\n",
    "- https://www.tensorflow.org/api_docs/python/tf/estimator/inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston_predict.csv  boston_test.csv  boston_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../data/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/boston_model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-5000\n",
      "INFO:tensorflow:Saving checkpoints for 5001 into /tmp/boston_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1713.56, step = 5001\n",
      "INFO:tensorflow:global_step/sec: 287.837\n",
      "INFO:tensorflow:loss = 4552.81, step = 5101 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.484\n",
      "INFO:tensorflow:loss = 4347.34, step = 5201 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.512\n",
      "INFO:tensorflow:loss = 4498.29, step = 5301 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.096\n",
      "INFO:tensorflow:loss = 4358.11, step = 5401 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.749\n",
      "INFO:tensorflow:loss = 6619.55, step = 5501 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.548\n",
      "INFO:tensorflow:loss = 4214.2, step = 5601 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.119\n",
      "INFO:tensorflow:loss = 4452.61, step = 5701 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.667\n",
      "INFO:tensorflow:loss = 4679.92, step = 5801 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.69\n",
      "INFO:tensorflow:loss = 6352.93, step = 5901 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 278.689\n",
      "INFO:tensorflow:loss = 3737.88, step = 6001 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.34\n",
      "INFO:tensorflow:loss = 5336.66, step = 6101 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.005\n",
      "INFO:tensorflow:loss = 4217.88, step = 6201 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.639\n",
      "INFO:tensorflow:loss = 3340.91, step = 6301 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.756\n",
      "INFO:tensorflow:loss = 4270.69, step = 6401 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 281.204\n",
      "INFO:tensorflow:loss = 5181.65, step = 6501 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.603\n",
      "INFO:tensorflow:loss = 2999.63, step = 6601 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.471\n",
      "INFO:tensorflow:loss = 1824.25, step = 6701 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.262\n",
      "INFO:tensorflow:loss = 4613.53, step = 6801 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.177\n",
      "INFO:tensorflow:loss = 1867.62, step = 6901 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.117\n",
      "INFO:tensorflow:loss = 5450.2, step = 7001 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.777\n",
      "INFO:tensorflow:loss = 3181.08, step = 7101 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.91\n",
      "INFO:tensorflow:loss = 3770.0, step = 7201 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.059\n",
      "INFO:tensorflow:loss = 3989.91, step = 7301 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.021\n",
      "INFO:tensorflow:loss = 3401.06, step = 7401 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.806\n",
      "INFO:tensorflow:loss = 3408.22, step = 7501 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 281.397\n",
      "INFO:tensorflow:loss = 6015.98, step = 7601 (0.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.762\n",
      "INFO:tensorflow:loss = 2068.23, step = 7701 (0.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.937\n",
      "INFO:tensorflow:loss = 4346.31, step = 7801 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.045\n",
      "INFO:tensorflow:loss = 4137.44, step = 7901 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.411\n",
      "INFO:tensorflow:loss = 3505.59, step = 8001 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.505\n",
      "INFO:tensorflow:loss = 5116.16, step = 8101 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.477\n",
      "INFO:tensorflow:loss = 4770.48, step = 8201 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.836\n",
      "INFO:tensorflow:loss = 3833.2, step = 8301 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.659\n",
      "INFO:tensorflow:loss = 2380.55, step = 8401 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 278.039\n",
      "INFO:tensorflow:loss = 4895.08, step = 8501 (0.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.438\n",
      "INFO:tensorflow:loss = 4350.42, step = 8601 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.761\n",
      "INFO:tensorflow:loss = 4644.19, step = 8701 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.543\n",
      "INFO:tensorflow:loss = 2239.86, step = 8801 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.521\n",
      "INFO:tensorflow:loss = 5448.2, step = 8901 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.588\n",
      "INFO:tensorflow:loss = 3133.03, step = 9001 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.597\n",
      "INFO:tensorflow:loss = 3496.03, step = 9101 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.003\n",
      "INFO:tensorflow:loss = 1565.17, step = 9201 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.899\n",
      "INFO:tensorflow:loss = 2542.55, step = 9301 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.067\n",
      "INFO:tensorflow:loss = 2475.57, step = 9401 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.019\n",
      "INFO:tensorflow:loss = 2830.1, step = 9501 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.913\n",
      "INFO:tensorflow:loss = 1851.5, step = 9601 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.132\n",
      "INFO:tensorflow:loss = 3332.6, step = 9701 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.116\n",
      "INFO:tensorflow:loss = 4781.61, step = 9801 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.116\n",
      "INFO:tensorflow:loss = 3258.09, step = 9901 (0.361 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/boston_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4723.48.\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-29-13:05:42\n",
      "INFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-10000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-29-13:05:42\n",
      "INFO:tensorflow:Saving dict for global step 10000: average_loss = 12.591, global_step = 10000, loss = 1259.1\n",
      "Loss: 1259.098389\n",
      "INFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-10000\n",
      "Predictions: [array([ 33.89645767], dtype=float32), array([ 18.13626671], dtype=float32), array([ 23.39229202], dtype=float32), array([ 34.82794571], dtype=float32), array([ 15.41587353], dtype=float32), array([ 19.01655006], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "COLUMNS = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\", \"age\",\n",
    "           \"dis\", \"tax\", \"ptratio\", \"medv\"]\n",
    "FEATURES = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\",\n",
    "            \"age\", \"dis\", \"tax\", \"ptratio\"]\n",
    "LABEL = \"medv\"\n",
    "\n",
    "\n",
    "def get_input_fn(data_set, num_epochs=None, shuffle=True):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),\n",
    "        y=pd.Series(data_set[LABEL].values),\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "\n",
    "# def main(unused_argv):\n",
    "    # Load datasets\n",
    "training_set = pd.read_csv(\"../../../data/boston_train.csv\", skipinitialspace=True,\n",
    "                         skiprows=1, names=COLUMNS)\n",
    "test_set = pd.read_csv(\"../../../data/boston_test.csv\", skipinitialspace=True,\n",
    "                     skiprows=1, names=COLUMNS)\n",
    "\n",
    "# Set of 6 examples for which to predict median house values\n",
    "prediction_set = pd.read_csv(\"../../../data/boston_predict.csv\", skipinitialspace=True,\n",
    "                           skiprows=1, names=COLUMNS)\n",
    "\n",
    "# Feature cols\n",
    "feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols,\n",
    "                                    hidden_units=[10, 10],\n",
    "                                    model_dir=\"/tmp/boston_model\")\n",
    "\n",
    "# Train\n",
    "regressor.train(input_fn=get_input_fn(training_set), steps=5000)\n",
    "\n",
    "# Evaluate loss over one epoch of test_set.\n",
    "ev = regressor.evaluate(\n",
    "  input_fn=get_input_fn(test_set, num_epochs=1, shuffle=False))\n",
    "loss_score = ev[\"loss\"]\n",
    "print(\"Loss: {0:f}\".format(loss_score))\n",
    "\n",
    "# Print out predictions over a slice of prediction_set.\n",
    "y = regressor.predict(\n",
    "  input_fn=get_input_fn(prediction_set, num_epochs=1, shuffle=False))\n",
    "# .predict() returns an iterator of dicts; convert to a list and print\n",
    "# predictions\n",
    "predictions = list(p[\"predictions\"] for p in itertools.islice(y, 6))\n",
    "print(\"Predictions: {}\".format(str(predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convert this notebook for Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 2011-11-11-DatasetHandling.ipynb to markdown\n",
      "[NbConvertApp] Writing 15277 bytes to 2011-11-11-DatasetHandling.md\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to markdown 2011-11-11-DatasetHandling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
