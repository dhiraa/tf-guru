{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Dataset Handling\"\n",
    "description: \"Handling Dataset with TnsorFlow Dataset APIs\"\n",
    "excerpt: \"Handling Dataset with TnsorFlow Dataset APIs\"\n",
    "date:   2017-11-11\n",
    "mathjax: true\n",
    "comments: true \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter notebook avaialble @ [https://github.com/dhiraa/tf-guru/blob/master/dataset/2017-11-11-DatasetHandling.ipynb](https://github.com/dhiraa/tf-guru/blob/master/dataset/2017-11-11-DatasetHandling.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling TextDataset with TensorFlow APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing vocab list with TF APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  1.3.0\n",
      "14 words into vocab.tsv\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lookup\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "print ('TensorFlow Version: ', tf.__version__)\n",
    "\n",
    "# Normally this takes the mean length of the words in the dataset documents\n",
    "MAX_DOCUMENT_LENGTH = 5  \n",
    "# Padding word that is used when a document has less words than the calculated mean length of the words\n",
    "PADWORD = 'ZYXW'\n",
    "\n",
    "# Assume each line to be an document\n",
    "lines = ['Simple',\n",
    "         'Some title', \n",
    "         'A longer title', \n",
    "         'An even longer title', \n",
    "         'This is longer than doc length']\n",
    "\n",
    "# Create vocabulary\n",
    "# min_frequency -> consider a word if and only it repeats for fiven count\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, \n",
    "                                                                     min_frequency=0)\n",
    "vocab_processor.fit(lines)\n",
    "\n",
    "#Create a file and store the words\n",
    "with gfile.Open('vocab_test.tsv', 'wb') as f:\n",
    "    f.write(\"{}\\n\".format(PADWORD))\n",
    "    for word, index in vocab_processor.vocabulary_._mapping.items():\n",
    "      f.write(\"{}\\n\".format(word))\n",
    "    \n",
    "VOCAB_SIZE = len(vocab_processor.vocabulary_)\n",
    "print ('{} words into vocab.tsv'.format(VOCAB_SIZE+1))\n",
    "\n",
    "EMBEDDING_SIZE = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZYXW\r\n",
      "<UNK>\r\n",
      "Simple\r\n",
      "Some\r\n",
      "title\r\n",
      "A\r\n",
      "longer\r\n",
      "An\r\n",
      "even\r\n",
      "This\r\n",
      "is\r\n",
      "than\r\n",
      "doc\r\n",
      "length\r\n"
     ]
    }
   ],
   "source": [
    "! cat vocab_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some title --> [3 4]\n"
     ]
    }
   ],
   "source": [
    "# can use the vocabulary to convert words to numbers\n",
    "table = lookup.index_table_from_file(\n",
    "  vocabulary_file='vocab_test.tsv', num_oov_buckets=1, vocab_size=None, default_value=-1)\n",
    "\n",
    "numbers = table.lookup(tf.constant(lines[1].split()))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Tables needs to be initialized before useing it\n",
    "    tf.tables_initializer().run()\n",
    "    print (\"{} --> {}\".format(lines[1], numbers.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles= [b'Simple' b'Some title' b'A longer title' b'An even longer title'\n",
      " b'This is longer than doc length'] (5,)\n",
      "--------------------------------------------------------\n",
      "words= SparseTensorValue(indices=array([[0, 0],\n",
      "       [1, 0],\n",
      "       [1, 1],\n",
      "       [2, 0],\n",
      "       [2, 1],\n",
      "       [2, 2],\n",
      "       [3, 0],\n",
      "       [3, 1],\n",
      "       [3, 2],\n",
      "       [3, 3],\n",
      "       [4, 0],\n",
      "       [4, 1],\n",
      "       [4, 2],\n",
      "       [4, 3],\n",
      "       [4, 4],\n",
      "       [4, 5]]), values=array([b'Simple', b'Some', b'title', b'A', b'longer', b'title', b'An',\n",
      "       b'even', b'longer', b'title', b'This', b'is', b'longer', b'than',\n",
      "       b'doc', b'length'], dtype=object), dense_shape=array([5, 6]))\n",
      "--------------------------------------------------------\n",
      "dense= [[b'Simple' b'ZYXW' b'ZYXW' b'ZYXW' b'ZYXW' b'ZYXW']\n",
      " [b'Some' b'title' b'ZYXW' b'ZYXW' b'ZYXW' b'ZYXW']\n",
      " [b'A' b'longer' b'title' b'ZYXW' b'ZYXW' b'ZYXW']\n",
      " [b'An' b'even' b'longer' b'title' b'ZYXW' b'ZYXW']\n",
      " [b'This' b'is' b'longer' b'than' b'doc' b'length']] (?, ?)\n",
      "--------------------------------------------------------\n",
      "numbers= [[ 2  0  0  0  0  0]\n",
      " [ 3  4  0  0  0  0]\n",
      " [ 5  6  4  0  0  0]\n",
      " [ 7  8  6  4  0  0]\n",
      " [ 9 10  6 11 12 13]] (?, ?)\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "sliced= [[ 2  0  0  0  0]\n",
      " [ 3  4  0  0  0]\n",
      " [ 5  6  4  0  0]\n",
      " [ 7  8  6  4  0]\n",
      " [ 9 10  6 11 12]] (?, 5)\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# string operations\n",
    "# Array of Docs -> Split it into Tokens/words \n",
    "#               -> Convert it into Dense Tensor apending PADWORD\n",
    "#               -> Table lookup \n",
    "#               -> Slice it to MAX_DOCUMENT_LENGTH\n",
    "titles = tf.constant(lines)\n",
    "words = tf.string_split(titles)\n",
    "densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "numbers = table.lookup(densewords)\n",
    "\n",
    "##Following extrasteps are taken care by above 'table.lookup'\n",
    "\n",
    "# now pad out with zeros and then slice to constant length\n",
    "# padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
    "# this takes care of documents with zero length also\n",
    "# padded = tf.pad(numbers, padding)\n",
    "\n",
    "sliced = tf.slice(numbers, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.tables_initializer().run()\n",
    "    print (\"titles=\", titles.eval(), titles.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"words=\", words.eval())\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"dense=\", densewords.eval(), densewords.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"numbers=\", numbers.eval(), numbers.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "#     print (\"padding=\", padding.eval(), padding.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "#     print (\"padded=\", padded.eval(), padded.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"sliced=\", sliced.eval(), sliced.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    \n",
    "    with tf.device('/cpu:0'), tf.name_scope(\"embed-layer\"):\n",
    "        # layer to take the words and convert them into vectors (embeddings)\n",
    "        # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "        # maps word indexes of the sequence into\n",
    "        # [batch_size, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE].\n",
    "        word_vectors = tf.contrib.layers.embed_sequence(sliced,\n",
    "                                                  vocab_size=VOCAB_SIZE,\n",
    "                                                  embed_dim=EMBEDDING_SIZE)\n",
    "\n",
    "        # [?, self.MAX_DOCUMENT_LENGTH, self.EMBEDDING_SIZE]\n",
    "        tf.logging.debug('words_embed={}'.format(word_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Estimators Inputs\n",
    "- https://www.tensorflow.org/api_docs/python/tf/estimator/inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../../../data/': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "COLUMNS = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\", \"age\",\n",
    "           \"dis\", \"tax\", \"ptratio\", \"medv\"]\n",
    "FEATURES = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\",\n",
    "            \"age\", \"dis\", \"tax\", \"ptratio\"]\n",
    "LABEL = \"medv\"\n",
    "\n",
    "\n",
    "def get_input_fn(data_set, num_epochs=None, shuffle=True):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),\n",
    "        y=pd.Series(data_set[LABEL].values),\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "\n",
    "# def main(unused_argv):\n",
    "    # Load datasets\n",
    "training_set = pd.read_csv(\"../data/boston_train.csv\", skipinitialspace=True,\n",
    "                         skiprows=1, names=COLUMNS)\n",
    "test_set = pd.read_csv(\"../data/boston_test.csv\", skipinitialspace=True,\n",
    "                     skiprows=1, names=COLUMNS)\n",
    "\n",
    "# Set of 6 examples for which to predict median house values\n",
    "prediction_set = pd.read_csv(\"../data/boston_predict.csv\", skipinitialspace=True,\n",
    "                           skiprows=1, names=COLUMNS)\n",
    "\n",
    "# Feature cols\n",
    "feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols,\n",
    "                                    hidden_units=[10, 10],\n",
    "                                    model_dir=\"/tmp/boston_model\")\n",
    "\n",
    "# Train\n",
    "regressor.train(input_fn=get_input_fn(training_set), steps=5000)\n",
    "\n",
    "# Evaluate loss over one epoch of test_set.\n",
    "ev = regressor.evaluate(\n",
    "  input_fn=get_input_fn(test_set, num_epochs=1, shuffle=False))\n",
    "loss_score = ev[\"loss\"]\n",
    "print(\"Loss: {0:f}\".format(loss_score))\n",
    "\n",
    "# Print out predictions over a slice of prediction_set.\n",
    "y = regressor.predict(\n",
    "  input_fn=get_input_fn(prediction_set, num_epochs=1, shuffle=False))\n",
    "# .predict() returns an iterator of dicts; convert to a list and print\n",
    "# predictions\n",
    "predictions = list(p[\"predictions\"] for p in itertools.islice(y, 6))\n",
    "print(\"Predictions: {}\".format(str(predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References: \n",
    "- https://medium.com/towards-data-science/how-to-do-text-classification-using-tensorflow-word-embeddings-and-cnn-edae13b3e575\n",
    "- https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/textclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 2017-11-11-DatasetHandling.ipynb to markdown\n",
      "[NbConvertApp] Writing 9278 bytes to ../docs/_posts/2017-11-11-DatasetHandling.md\n"
     ]
    }
   ],
   "source": [
    "# Convert this notebook for Docs\n",
    "! jupyter nbconvert --to markdown --output-dir ../docs/_posts 2017-11-11-DatasetHandling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
