---
layout: post
title:  "tf.nn.*rnn* APIs"
description: "Some Intersting Facts About RNN APIs"
excerpt: "Some Intersting Facts About RNN APIs"
date:   2017-11-11
mathjax: true
comments: true 
---

**Jupyter notebook avaialble @ [https://github.com/dhiraa/tf-guru/blob/master/dataset/2017-11-11-tf.nn.rnn-APIs.ipynb](https://github.com/dhiraa/tf-guru/blob/master/dataset/2017-11-11-tf.nn.rnn-APIs.ipynb)**

# static_rnn vs. dynamic_rnn

You may have noticed that Tensorflow contains two different functions for RNNs: [tf.nn.static_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn) and [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn). Which one to use?


Internally, tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified.

`tf.nn.dynamic_rnn` solves this. It uses a `tf.While` loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size. What about performance? You may think the static rnn is faster than its dynamic counterpart because it pre-builds the graph. In my experience that’s not the case.

In short, just use `tf.nn.dynamic_rnn`. 


```python
import tensorflow as tf
import numpy as np
```


```python
tf.reset_default_graph()
sess = tf.Session()

BATCH_SIZE = 2
SEQUENCE_LENGTH = 10
NUMBER_OF_WORDS = 8
# Create input data
X = np.random.randn(BATCH_SIZE, SEQUENCE_LENGTH, NUMBER_OF_WORDS)

# The second example is of length 6 
X[1,6:] = 0
X_lengths = [10, 6]
```


```python
with tf.device(":/cpu:0"):
    data = tf.placeholder(tf.float64, [None,  SEQUENCE_LENGTH, NUMBER_OF_WORDS])
    cell = tf.nn.rnn_cell.LSTMCell(num_units=8, state_is_tuple=True)

    # defining initial state
    initial_state = cell.zero_state(BATCH_SIZE, dtype=tf.float64)

    encode = tf.nn.dynamic_rnn(
        cell=cell,
        dtype=tf.float64,
        sequence_length=X_lengths,
        inputs=data,
        initial_state=initial_state)
```


```python
sess.run(tf.global_variables_initializer())
outputs, last_states = sess.run(encode, feed_dict={data: X})
```


```python
outputs
```




    array([[[-0.0283708 , -0.02738941,  0.0755726 , -0.08008824,  0.01081016,
             -0.03213166, -0.0536146 , -0.15033308],
            [-0.07012742,  0.07191048,  0.16121555, -0.06183664,  0.10730578,
              0.05112168,  0.20581951, -0.03912028],
            [ 0.1738402 ,  0.26620728,  0.10301632,  0.13077066,  0.0349601 ,
              0.0586684 ,  0.27258061,  0.32184957],
            [-0.00453193,  0.05837649,  0.00355916, -0.08189783,  0.01207369,
              0.08217721,  0.09261072,  0.1040738 ],
            [ 0.07379494,  0.06887853,  0.02938879, -0.04013838, -0.01955719,
              0.02584432,  0.09734795,  0.12879024],
            [ 0.20445797,  0.04512449, -0.03814287,  0.10622528,  0.02263981,
             -0.17649335,  0.24971871,  0.36151143],
            [ 0.20608251,  0.13480439,  0.10275332,  0.04439773, -0.00303557,
             -0.27451886,  0.17191344,  0.31303354],
            [ 0.32365047,  0.48470812,  0.17407803,  0.2052482 , -0.02023543,
             -0.17407063,  0.29517553,  0.48837349],
            [ 0.1345457 ,  0.33321533,  0.28944614,  0.09551912,  0.03292939,
             -0.20709579,  0.05701367,  0.12379222],
            [ 0.17016332,  0.06186617,  0.15829436,  0.16037617, -0.15841691,
             -0.3717137 ,  0.11021478,  0.22602701]],
    
           [[ 0.04793363,  0.12344369, -0.1783383 ,  0.04842179,  0.15124708,
              0.05783913,  0.09932703, -0.04334414],
            [-0.08500263,  0.29018905, -0.13871866,  0.14451168, -0.01455011,
              0.03964509,  0.2207585 ,  0.1708107 ],
            [-0.07214073,  0.21678313, -0.11355709,  0.04147981,  0.11652134,
              0.03886285,  0.04392174,  0.20238952],
            [-0.11234446,  0.16728768, -0.14256807, -0.09604893, -0.05138749,
              0.1120749 ,  0.12973008,  0.26944325],
            [-0.08622686,  0.20366575, -0.0149762 , -0.14269766, -0.32314589,
              0.02864775,  0.06804786,  0.24432493],
            [-0.05230012,  0.42949286, -0.07228138, -0.04229116, -0.11640012,
             -0.06838009,  0.30524024,  0.28048882],
            [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
              0.        ,  0.        ,  0.        ],
            [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
              0.        ,  0.        ,  0.        ],
            [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
              0.        ,  0.        ,  0.        ],
            [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
              0.        ,  0.        ,  0.        ]]])




```python
last_states
```




    LSTMStateTuple(c=array([[ 0.26831959,  0.33156717,  0.33671164,  0.2987655 , -0.24417013,
            -0.92449115,  0.30798403,  0.61355526],
           [-0.14194088,  0.71295599, -0.10051528, -0.11628008, -0.26955969,
            -0.15300985,  0.45661472,  0.54125506]]), h=array([[ 0.17016332,  0.06186617,  0.15829436,  0.16037617, -0.15841691,
            -0.3717137 ,  0.11021478,  0.22602701],
           [-0.05230012,  0.42949286, -0.07228138, -0.04229116, -0.11640012,
            -0.06838009,  0.30524024,  0.28048882]]))



#  Bidirectional RNNs

When using a standard RNN to make predictions we are only taking the “past” into account. For certain tasks this makes sense (e.g. predicting the next word), but for some tasks it would be useful to take both the past and the future into account. Think of a tagging task, like part-of-speech tagging, where we want to assign a tag to each word in a sentence. Here we already know the full sequence of words, and for each word we want to take not only the words to the left (past) but also the words to the right (future) into account when making a prediction. Bidirectional RNNs do exactly that. A bidirectional RNN is a combination of two RNNs – one runs forward from “left to right” and one runs backward from “right to left”. These are commonly used for tagging tasks, or when we want to embed a sequence into a fixed-length vector (beyond the scope of this post).

Just like for standard RNNs, Tensorflow has [static](https://www.tensorflow.org/api_docs/python/tf/nn/static_bidirectional_rnn) and [dynamic](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn) versions of the bidirectional RNN. The bidirectional_dynamic_rnn is preferred over the static bidirectional_rnn.

The key differences of the bidirectional RNN functions are that they take a separate cell argument for both the forward and backward RNN, and that they return separate outputs and states for both the forward and backward RNN:


```python
with tf.device(":/cpu:0"):
    data = tf.placeholder(tf.float64, [None,  SEQUENCE_LENGTH, NUMBER_OF_WORDS])
    cell = tf.nn.rnn_cell.LSTMCell(num_units=4, state_is_tuple=True)

    # defining initial state
    initial_state = cell.zero_state(BATCH_SIZE, dtype=tf.float64)

    encode = tf.nn.dynamic_rnn(
        cell=cell,
        dtype=tf.float64,
        sequence_length=X_lengths,
        inputs=data,
        initial_state=initial_state)
```


```python
sess.run(tf.global_variables_initializer())
outputs, states = sess.run(encode, feed_dict={data: X})
```


```python
output_fw, output_bw = outputs
states_fw, states_bw = states
```


```python
#SEQUENCE_LENGTH, LSTM_HIDDEN_SIZE
output_bw.shape
```




    (10, 4)




```python
##BATCH_SIZE, LSTM_HIDDEN_SIZE
states_bw.shape
```




    (2, 4)




```python
# Convert this notebook for Docs
! jupyter nbconvert --to markdown --output-dir ../docs/_posts 2017-11-11-tf.nn.rnn-APIs.ipynb
```

    [NbConvertApp] Converting notebook 2017-11-11-tf.nn.rnn-APIs.ipynb to markdown
    [NbConvertApp] Writing 8123 bytes to ../docs/_posts/2017-11-11-tf.nn.rnn-APIs.md



```python

```
